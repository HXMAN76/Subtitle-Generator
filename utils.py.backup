from moviepy import (
  AudioFileClip
)
import torch
import whisper

model = whisper.load_model("tiny")

def extract_audio_from_video(video_path, audio_path):
  """Extract audio from a video file and save it as an audio file.
  Args:
      video_path (str): Path to the input video file.
      audio_path (str): Path to save the extracted audio file.
  """
  video_clip = AudioFileClip(video_path)
  video_clip.write_audiofile(audio_path)
  video_clip.close()

def convert(input_video):
  output_audio = "temp/"+ input_video.rsplit('.', 1)[0] + '.mp3'
  extract_audio_from_video(input_video, output_audio)
  return output_audio

def voice_detect(audio_path):
  # Placeholder for voice detection logic
  torch.set_num_threads(1)

  model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')
  (get_speech_timestamps, _, read_audio, _, _) = utils

  wav = read_audio(audio_path)
  speech_timestamps = get_speech_timestamps(
    wav,
    model,
    return_seconds=True,  # Return speech timestamps in seconds (default is samples)
  )
  return speech_timestamps

def segment_audio(audio_path, speech_timestamps):
  # Placeholder for audio segmentation logic
  from pydub import AudioSegment
  audio = AudioSegment.from_file(audio_path)
  segments = []
  for i, ts in enumerate(speech_timestamps):
    start_ms = int(ts['start'] * 1000)  # Convert to milliseconds
    end_ms = int(ts['end'] * 1000)      # Convert to milliseconds
    segment = audio[start_ms:end_ms]
    segment_path = f"temp/voice/segment_{i}.mp3"
    segment.export(segment_path, format="mp3")
    segments.append(segment_path)
  return segments

def process_audio_segments(segments):
  # Placeholder for processing each audio segment
  results = []
  for segment in segments:
    print(f"Processing segment: {segment}")
    result = model.transcribe(segment)
    print(f"Transcription: {result['text']}")
    results.append(result)
  return results