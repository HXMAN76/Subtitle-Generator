# üé¨ Subtitle Generator & Translator

A production-ready, offline subtitle generation and translation application using **Whisper** for speech-to-text, **Silero VAD** for voice activity detection, and a custom-trained neural translation model.

---

## ‚ú® Features

- **Audio Extraction** - Extract audio from video files (MP4, AVI, MKV, MOV, WebM, FLV)
- **Voice Activity Detection** - Detect speech segments using Silero VAD
- **Speech-to-Text** - Transcribe audio using OpenAI Whisper
- **Custom Translation** - Translate subtitles using a custom-trained neural model (no cloud APIs)
- **Subtitle Generation** - Generate SRT and VTT subtitle files
- **Offline Operation** - Runs completely locally with no internet connection required
- **Production Structure** - Clean, modular, and maintainable codebase

---

## üìÅ Project Structure

```
Subtitle-Generator/
‚îú‚îÄ‚îÄ app.py                          # Main application entry point
‚îú‚îÄ‚îÄ config.py                       # Configuration settings
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ README.md                       # Documentation (this file)
‚îÇ
‚îú‚îÄ‚îÄ src/                            # Source code modules
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ audio_processor.py          # Audio extraction and segmentation
‚îÇ   ‚îú‚îÄ‚îÄ vad.py                      # Voice activity detection
‚îÇ   ‚îú‚îÄ‚îÄ transcriber.py              # Whisper transcription
‚îÇ   ‚îú‚îÄ‚îÄ translator.py               # Custom translation model
‚îÇ   ‚îî‚îÄ‚îÄ subtitle_generator.py       # SRT/VTT generation
‚îÇ
‚îú‚îÄ‚îÄ models/                         # Trained models
‚îÇ   ‚îî‚îÄ‚îÄ translation/                # Translation model files
‚îÇ       ‚îú‚îÄ‚îÄ model.pt
‚îÇ       ‚îî‚îÄ‚îÄ vocab.json
‚îÇ
‚îú‚îÄ‚îÄ data/                           # Training data
‚îÇ   ‚îú‚îÄ‚îÄ raw/                        # Raw translation datasets
‚îÇ   ‚îî‚îÄ‚îÄ processed/                  # Processed training data
‚îÇ       ‚îî‚îÄ‚îÄ train_data.json
‚îÇ
‚îú‚îÄ‚îÄ scripts/                        # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ train_translator.py         # Train translation model
‚îÇ   ‚îî‚îÄ‚îÄ download_dataset.py         # Download training data
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                      # Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ data_exploration.ipynb      # Data exploration notebook
‚îÇ
‚îú‚îÄ‚îÄ tests/                          # Test files
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_app.py                 # Application tests
‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py               # Test utilities
‚îÇ
‚îú‚îÄ‚îÄ examples/                       # Example files
‚îÇ   ‚îî‚îÄ‚îÄ sample_video.mp4            # Sample video for testing
‚îÇ
‚îú‚îÄ‚îÄ output/                         # Generated subtitle files
‚îî‚îÄ‚îÄ temp/                           # Temporary files
    ‚îî‚îÄ‚îÄ voice/                      # Segmented audio files
```

---

## üöÄ Quick Start

### Prerequisites

- **Python 3.8+**
- **FFmpeg** (for audio processing)

### 1. Install FFmpeg

| Platform | Command |
|----------|---------|
| **Linux** | `sudo apt-get install ffmpeg` |
| **macOS** | `brew install ffmpeg` |
| **Windows** | Download from [ffmpeg.org](https://ffmpeg.org/download.html) |

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Run the Application

```bash
python app.py
```

> **Note:** Update the `video_path` in `app.py` to point to your video file before running.

### 4. Output

Subtitles will be generated in the `output/` folder:
- `video_name_original.srt` ‚Äî Original transcription
- `video_name_es.srt` ‚Äî Translated version (if translation model is trained)

---

## ‚öôÔ∏è Configuration

Edit `config.py` to customize settings:

```python
# Whisper settings
WHISPER_MODEL_SIZE = "tiny"     # Options: tiny, base, small, medium, large
WHISPER_DEVICE = "cpu"          # Options: cpu, cuda

# Languages
SOURCE_LANGUAGE = "en"
TARGET_LANGUAGE = "es"          # Change as needed

# Subtitle settings
SUBTITLE_FORMAT = "srt"         # Options: srt, vtt

# VAD sensitivity
VAD_THRESHOLD = 0.5             # Range: 0.0 to 1.0
```

### Whisper Model Comparison

| Model | Speed | Accuracy | Memory | Use Case |
|-------|-------|----------|--------|----------|
| `tiny` | ‚ö° Fastest | Low | ~1 GB | Testing |
| `base` | Fast | Good | ~1.5 GB | **Production** |
| `small` | Medium | Better | ~2 GB | Quality |
| `medium` | Slow | High | ~4 GB | Accuracy |
| `large` | Slowest | Highest | ~8 GB | Best quality |

---

## üìñ Advanced Usage

### Programmatic API

```python
from src.audio_processor import AudioProcessor
from src.vad import VoiceActivityDetector
from src.transcriber import Transcriber
from src.translator import Translator
from src.subtitle_generator import SubtitleGenerator

# Initialize components
audio_processor = AudioProcessor()
vad = VoiceActivityDetector()
transcriber = Transcriber()
translator = Translator()
subtitle_gen = SubtitleGenerator()

# Process video
audio_path = audio_processor.convert_video_to_audio("video.mp4")
speech_timestamps = vad.detect_speech(audio_path)
segments = audio_processor.segment_audio(audio_path, speech_timestamps)
transcriptions = transcriber.transcribe_segments(segments)

# Generate subtitles
subtitle_gen.generate_subtitles(transcriptions, "output", format="srt")

# Translate (if model is trained)
translated = translator.translate_subtitles(transcriptions)
subtitle_gen.generate_subtitles(translated, "output_translated", format="srt")
```

### Using the SubtitleApp Class

```python
from app import SubtitleApp

app = SubtitleApp()
results = app.process_video(
    video_path="your_video.mp4",
    translate=True  # Enable translation
)
```

### Batch Processing

```python
import glob
from app import SubtitleApp

app = SubtitleApp()

# Process all MP4 files
for video in glob.glob("*.mp4"):
    print(f"Processing {video}...")
    app.process_video(video, translate=False)
```

---

## üß† Training the Translation Model

### 1. Prepare Training Data

Create a JSON file with parallel sentences at `data/processed/train_data.json`:

```json
[
  {"source": "Hello world", "target": "Hola mundo"},
  {"source": "Good morning", "target": "Buenos d√≠as"},
  {"source": "How are you?", "target": "¬øC√≥mo est√°s?"}
]
```

**Get Training Data:**
- Download parallel corpora from [OPUS](https://opus.nlpl.eu/)
- Use [Tatoeba](https://tatoeba.org/) for sentence pairs
- Create your own dataset

### 2. Train the Model

```bash
python scripts/train_translator.py
```

Training time depends on dataset size (typically 10-30 minutes for 10K sentence pairs).

### 3. Model Architecture

- **Encoder:** Bidirectional LSTM
- **Decoder:** LSTM with attention
- **Embeddings:** 256 dimensions
- **Hidden:** 512 dimensions
- **Layers:** 2 LSTM layers

The trained model will be saved to `models/translation/`.

---

## üì¶ Building as Executable (.exe)

### Quick Build

```bash
pip install pyinstaller
pyinstaller --onefile --name SubtitleGenerator --add-data "src:src" --add-data "config.py:." --add-data "models:models" --hidden-import=whisper --hidden-import=torch app.py
```

### Using Spec File

Create `subtitle_generator.spec`:

```python
# -*- mode: python ; coding: utf-8 -*-

block_cipher = None

a = Analysis(
    ['app.py'],
    pathex=[],
    binaries=[],
    datas=[
        ('src', 'src'),
        ('models', 'models'),
        ('config.py', '.'),
    ],
    hiddenimports=[
        'whisper',
        'torch',
        'moviepy',
        'pydub',
        'numpy',
        'scipy',
        'tiktoken',
        'regex',
    ],
    hookspath=[],
    hooksconfig={},
    runtime_hooks=[],
    excludes=[],
    win_no_prefer_redirects=False,
    win_private_assemblies=False,
    cipher=block_cipher,
    noarchive=False,
)

pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)

exe = EXE(
    pyz,
    a.scripts,
    a.binaries,
    a.zipfiles,
    a.datas,
    [],
    name='SubtitleGenerator',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    console=True,
    icon='icon.ico'  # Optional
)
```

Then build:

```bash
pyinstaller subtitle_generator.spec
```

### Distribution Package

```
SubtitleGenerator/
‚îú‚îÄ‚îÄ SubtitleGenerator.exe       # Main executable
‚îú‚îÄ‚îÄ ffmpeg.exe                  # Required
‚îú‚îÄ‚îÄ ffprobe.exe                 # Required
‚îú‚îÄ‚îÄ models/                     # Model files
‚îÇ   ‚îî‚îÄ‚îÄ translation/
‚îÇ       ‚îú‚îÄ‚îÄ model.pt
‚îÇ       ‚îî‚îÄ‚îÄ vocab.json
‚îú‚îÄ‚îÄ output/                     # Empty folder
‚îú‚îÄ‚îÄ temp/                       # Empty folder
‚îî‚îÄ‚îÄ README.txt                  # Usage instructions
```

### Optimizations

**Reduce File Size:**
- Use smaller Whisper model (tiny or base)
- Install CPU-only PyTorch: `pip install torch --index-url https://download.pytorch.org/whl/cpu`
- Enable UPX compression

**Pre-download Models:**
```python
import whisper
import torch

whisper.load_model("tiny")  # Downloads once
model, utils = torch.hub.load('snakers4/silero-vad', 'silero_vad')
```

---

## üîß Troubleshooting

| Issue | Solution |
|-------|----------|
| **FFmpeg not found** | Install FFmpeg and ensure it's in your PATH |
| **Out of memory** | Use a smaller Whisper model (tiny or base) |
| **CUDA error** | Set `WHISPER_DEVICE = "cpu"` in `config.py` |
| **Translation returns original text** | Train the translation model first |
| **Poor transcription quality** | Use larger Whisper model or improve audio quality |

---

## üéØ Performance Tips

1. **Use GPU** - If you have CUDA, set `WHISPER_DEVICE = "cuda"` in config
2. **Smaller Models** - Use "tiny" for quick testing, "base" for production
3. **VAD Tuning** - Adjust `VAD_THRESHOLD` (0.3-0.7) based on audio quality
4. **Batch Processing** - Process multiple videos in sequence to reuse loaded models

### Benchmark (1-minute video, Intel i7, 16GB RAM)

| Model | Time |
|-------|------|
| tiny | ~5 seconds |
| base | ~15 seconds |
| small | ~30 seconds |
| medium | ~60 seconds |
| large | ~2-3 minutes |

---

## üîß Dependencies

### Core
- **moviepy** - Video/audio processing
- **openai-whisper** - Speech-to-text transcription
- **torch** - Neural network framework
- **pydub** - Audio manipulation
- **silero-vad** - Voice activity detection

### System
- **FFmpeg** - Required for audio processing
- **Python 3.8+** - Development

---

## üìä Supported Formats

### Video Input
- MP4 (recommended)
- AVI, MKV, MOV
- WebM, FLV

### Subtitle Output
- SRT (SubRip)
- VTT (WebVTT)

---

## üéØ Roadmap

- [ ] GUI interface
- [ ] Batch processing CLI
- [ ] Multiple language support
- [ ] Real-time subtitling
- [ ] Custom model fine-tuning interface
- [ ] Subtitle editing capabilities

---

## ü§ù Contributing

Contributions welcome! Please feel free to submit a Pull Request.

---

## üìù License

MIT License - feel free to use for personal or commercial projects.

---

## üìû Support

For issues or questions, please open an issue on GitHub.

---

**Note:** This application runs completely offline. Initial setup requires internet to download Whisper models and dependencies, but afterward, it works without any cloud connections.
