# NLP Literature Review: Subtitle Generation & Translation Systems

**Team A - Batch 9**  
**Domain**: Automated Subtitle Generation and Neural Machine Translation

---

## ğŸ“š Comprehensive Literature Review

| Study (Author, Year) | Research Focus | Architecture / Methodology | Benchmarks & Evaluation | Our Project Comparison |
|---------------------|----------------|---------------------------|------------------------|----------------------|
| **Prabhakar et al. (2025)** | Automated Subtitling & Translation (Direct relevance) | **Whisper (OpenAI)** for Speech-to-Text + **Helsinki-NLP models** for Translation | â€¢ Comparison against standard Whisper models<br>â€¢ Accuracy validation on large-scale subtitle datasets | âœ… **Similar approach**: We use faster-whisper (optimized)<br>âŒ **Different NMT**: We use custom 60M Transformer vs Helsinki models<br>âœ… **Edge**: 11 Indic languages with offline deployment |
| **Papi et al. (2023)** | Simultaneous Translation & Subtitling | **Triangle Transformer**: One encoder, two decoders for simultaneous output | â€¢ BLEU, SubER, and Sigma scores<br>â€¢ Tested on MuST-Cinema corpus | âŒ **Different**: Sequential pipeline (ASR â†’ NMT) vs simultaneous<br>âœ… **Advantage**: Lower latency for post-processing<br>ğŸ”„ **Trade-off**: Not real-time but more accurate |
| **Rajaboina & Sariki (2025)** | Enhanced Contextual Understanding | **Hybrid BERT-CNN-LSTM**: BERT (context) + CNN (features) + LSTM (sequence) | â€¢ Contextual accuracy (metrics not detailed) | âŒ **Different architecture**: Pure Transformer vs hybrid<br>âœ… **Advantage**: Lighter model (60M vs BERT-based ~340M)<br>âœ… **Efficiency**: Faster inference |
| **Anand et al. (2025)** | Real-Time Live Video Subtitling | **Hybrid Bidirectional LSTM + Transformer** | â€¢ Word Accuracy Rate<br>â€¢ Latency measurement (real-time focus) | âŒ **Not real-time**: Background processing pipeline<br>âœ… **Better accuracy**: Full-audio mode vs streaming<br>ğŸ¯ **Use case**: Post-production vs live streaming |
| **Poncelet & Hamme (2025)** | Broadcast Media Transcription | **MultiTransformer Decoder**: End-to-end with cascaded encoders | â€¢ ASR and subtitle generation accuracy | âœ… **Similar**: Transformer-based architecture<br>âŒ **Different**: Cascaded encoders vs standard 6-layer<br>âœ… **Advantage**: Simpler, proven architecture |
| **Yu et al. (2025)** | End-to-End Extraction (Video-based) | **Vision-Language Models**: Vision encoder + InterleavedVT + LLM | â€¢ Tested on ViSa dataset (2.5M videos)<br>â€¢ Compared vs open-source tools and LVLMs | âŒ **Different modality**: Audio-only vs multimodal<br>âœ… **Focused**: Specialized for audio subtitles<br>âœ… **Efficiency**: No visual processing overhead |
| **Penyameen et al. (2025)** | Multilingual Video Transcription | **Whisper (OpenAI)** + FFmpeg + MoviePy | â€¢ System performance validation (metrics not specified) | âœ… **Very similar stack**: Whisper + FFmpeg + MoviePy<br>âœ… **Enhancement**: faster-whisper (3-4x speedup)<br>âœ… **Addition**: Custom NMT for Indic languages |
| **Google Translate WMT 2025** | LLM-based NMT Refinement | **Gemma 3 LLM** with fine-tuning and reinforcement learning | â€¢ WMT benchmarks<br>â€¢ Fluent vs literal style control | âŒ **Model size**: 60M params vs Gemma 3 (7B+)<br>âœ… **Deployment**: Offline-capable vs cloud-only<br>âŒ **Scope**: Task-specific vs general-purpose |
| **VNJPTranslate (2025)** | Low-Resource Language Pairs (Vietnamese-Japanese) | **LLMs with QLoRA** for efficient fine-tuning + synthetic data | â€¢ Low-resource pair performance | âœ… **Similar challenge**: Low-resource Indic languages<br>âœ… **Different approach**: From-scratch training vs fine-tuning<br>âœ… **Scale**: 11 languages vs 1 pair |
| **Sony AI (ACL 2025)** | Domain-Adaptive Translation | **Graph Neural Networks** for idiomatic translation + **Multi-Armed Bandit** for model selection | â€¢ Domain-specific accuracy<br>â€¢ African low-resource languages | âŒ **No dynamic selection**: Single model per language<br>âœ… **Simpler**: Standard Transformer<br>ğŸ”„ **Future**: Could add model selection |
| **Multimodal NMT (2025)** | Image Caption Translation | **CNN + RNN with attention** for visual + textual inputs | â€¢ Under-resourced language performance | âŒ **Audio-only**: No visual modality<br>âœ… **Focused**: Specialized for speech<br>âœ… **Efficiency**: Lower computational cost |
| **AI-Powered Subtitle Management (Feb 2025)** | End-to-End Subtitle System | **ASR + MT + Segmentation + Formatting** integrated system | â€¢ Multi-language accuracy<br>â€¢ Synchronization metrics | âœ… **Complete pipeline**: Similar integrated approach<br>âœ… **Enhancement**: Custom NMT vs generic MT<br>âœ… **REST API**: Production-ready with FastAPI |

---

## ğŸ¯ Our Project: Technical Specifications

### System Overview

**Name**: Multi-Language Subtitle Generator & Translator v2.0.0  
**Type**: Offline, Production-Ready REST API System

### Core Components

| Component | Technology | Specifications |
|-----------|-----------|----------------|
| **Speech Recognition** | faster-whisper (CTranslate2) | â€¢ 3-4x faster than OpenAI Whisper<br>â€¢ GPU/CPU auto-detection<br>â€¢ Models: tiny to large-v3 |
| **Translation** | Custom Transformer NMT | â€¢ 60.52M parameters per language<br>â€¢ 11 language-specific models<br>â€¢ Lazy loading architecture |
| **Tokenizer** | SentencePiece (Shared) | â€¢ 32K vocabulary<br>â€¢ 12 language tags<br>â€¢ Single tokenizer for all languages |
| **API** | FastAPI | â€¢ Background job processing<br>â€¢ Swagger/OpenAPI docs<br>â€¢ Multi-language support endpoints |

### Supported Languages

**Source**: English (en)  
**Targets**: Assamese (as), Bengali (bn), Gujarati (gu), Hindi (hi), Kannada (kn), Malayalam (ml), Marathi (mr), Odia (or), Punjabi (pa), Tamil (ta), Telugu (te)

### Performance Metrics

| Language | BLEU Score | Status | vs IndicTrans2 |
|----------|------------|--------|----------------|
| Hindi (hi) | 39.33 | âœ… Good | -8.87 |
| Gujarati (gu) | 39.76 | âœ… Competitive | -1.44 |
| Odia (or) | 63.89 | ğŸ” Verify | +26.79 |
| Punjabi (pa) | 34.93 | âœ… Acceptable | -4.67 |
| Assamese (as) | 14.37 | âš ï¸ Needs work | -14.93 |
| Kannada (kn) | 12.14 | âš ï¸ Retrain | -21.06 |
| Malayalam (ml) | 15.97 | âš ï¸ Retrain | -18.13 |
| Tamil (ta) | 15.44 | âš ï¸ Retrain | -20.36 |

---

## ğŸ” Research Gaps Our Project Addresses

### 1. **Offline Indic Language Translation** ğŸŒ

**Gap**: Most commercial systems (Google Translate, Azure) require internet connectivity. Low-resource Indic languages have limited offline support.

**Our Solution**:
- âœ… Fully offline operation
- âœ… 11 Indic languages with custom-trained models
- âœ… Trained on 49.6M sentence pairs (Samanantar dataset)
- âœ… Deployable on local infrastructure

**Impact**: Enables subtitle generation in regions with limited internet or data privacy requirements.

---

### 2. **Memory-Efficient Multi-Language Systems** ğŸ’¾

**Gap**: Loading multiple large translation models (1B+ params) simultaneously is memory-prohibitive for edge deployment.

**Our Solution**:
- âœ… **Lazy loading**: Models loaded on-demand
- âœ… **Shared tokenizer**: Single 5MB tokenizer for all 11 languages
- âœ… **Compact models**: 60M params per model (20x smaller than IndicTrans2)
- âœ… **Selective loading**: Load only required languages

**Impact**: Enables deployment on consumer-grade GPUs (8GB VRAM) with all 11 languages available.

---

### 3. **Production-Ready Subtitle Pipeline** ğŸ¬

**Gap**: Academic research often focuses on individual components (ASR or NMT) without end-to-end integration.

**Our Solution**:
- âœ… **Complete pipeline**: Video â†’ Audio â†’ Transcription â†’ Translation â†’ SRT/VTT
- âœ… **REST API**: FastAPI with background job processing
- âœ… **Format support**: SRT and VTT subtitle formats
- âœ… **Progress tracking**: Real-time job status with `/jobs/{id}` endpoint

**Impact**: Can be directly deployed in production workflows without additional integration work.

---

### 4. **Optimized Inference Speed** âš¡

**Gap**: Standard Whisper and Transformer models have high inference latency for long videos.

**Our Solution**:
- âœ… **faster-whisper**: CTranslate2-optimized (3-4x speedup)
- âœ… **Full-audio mode**: Process entire audio in one pass (no chunking overhead)
- âœ… **Batch translation**: GPU-optimized subtitle translation
- âœ… **Async processing**: Non-blocking API with background tasks

**Impact**: Process 2-hour video in 25-40 minutes (GPU) vs 90-120 minutes with standard tools.

---

### 5. **Language-Specific Model Customization** ğŸ”§

**Gap**: Multilingual models (mBART, NLLB) suffer from "curse of multilinguality" where performance degrades with more languages.

**Our Solution**:
- âœ… **Per-language models**: Separate 60M model for each target language
- âœ… **Specialized training**: Language-specific fine-tuning
- âœ… **Flexible updates**: Can retrain individual languages without affecting others
- âœ… **Dravidian strategy**: Planned specialized tokenizer for kn/ml/ta

**Impact**: Better performance for specific language pairs vs generic multilingual models.

---

### 6. **Transparent Evaluation & Benchmarking** ğŸ“Š

**Gap**: Many commercial systems don't publish detailed performance metrics or comparisons.

**Our Solution**:
- âœ… **Open evaluation**: BLEU, METEOR, chrF metrics on test sets
- âœ… **Baseline comparisons**: vs IndicTrans, Google Translate (planned)
- âœ… **Reproducible**: All evaluation scripts and datasets documented
- âœ… **Gap analysis**: Identified weaknesses (Dravidian languages) with retraining plan

**Impact**: Transparent, research-grade quality assessment enabling continuous improvement.

---

### 7. **Dravidian Language Focus** ğŸ¯

**Gap**: Dravidian languages (Kannada, Malayalam, Tamil, Telugu) are underrepresented in NMT research compared to Indo-Aryan languages.

**Our Solution**:
- âœ… **Explicit support**: 4 Dravidian languages included
- âœ… **Identified tokenization issue**: Planned separate Dravidian tokenizer
- âœ… **Retraining strategy**: Documented approach to improve BLEU by 10-15 points
- âœ… **Dataset**: Large-scale training data (4-5M pairs per language)

**Impact**: Addresses critical gap in South Indian language technology.

---

## ğŸ“Š Comparative Analysis

### Strengths vs Competitors

| Aspect | Our Project | Competitors |
|--------|-------------|-------------|
| **Offline Operation** | âœ… Full offline | âŒ Most require cloud |
| **Indic Languages** | âœ… 11 with custom models | âš ï¸ Generic or limited |
| **Model Size** | âœ… 60M (efficient) | âŒ 1B+ params |
| **Deployment** | âœ… Consumer GPU (8GB) | âŒ High-end GPUs |
| **API** | âœ… REST ready | âš ï¸ Varies |

### Challenges

| Aspect | Our Project | Industry |
|--------|-------------|----------|
| **Quality** | âš ï¸ BLEU 39 (hi), 14-16 (Dravidian) | âœ… BLEU 48+ |
| **Real-Time** | âŒ Batch only | âœ… Some support live |
| **LLM** | âŒ Traditional Transformer | âœ… Gemma, GPT-based |
